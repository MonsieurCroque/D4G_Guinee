{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pipeline-CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3qr0ZB6LUQ4",
        "colab_type": "code",
        "outputId": "2aa6f311-a1bc-4f1f-8c14-6a318f89e6a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBZbfd5FTk5j",
        "colab_type": "code",
        "outputId": "3e2caecb-d7d8-465c-e2d4-f21f8ce365c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!pip install pyrsgis"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyrsgis\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/80/a74e3d968e2b74450d7de85dbd13785c73e8580a956fa56d14973266a9bf/pyrsgis-0.3.1-py3-none-any.whl\n",
            "Installing collected packages: pyrsgis\n",
            "Successfully installed pyrsgis-0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idlZ5PZxJi8H",
        "colab_type": "code",
        "outputId": "38f357e6-8de5-498b-f137-6f49f5c53b0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "######################### Import des libraries #######################\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from pyrsgis import raster\n",
        "from pyrsgis.convert import changeDimension\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import scipy.signal\n",
        "import scipy.ndimage\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning! matplotlib_scalebar library not found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44yhFLpUJjdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################## Parameters ############################\n",
        "\n",
        "stop = 1000\n",
        "epochs = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N830fS93Jof2",
        "colab_type": "code",
        "outputId": "38b4115b-e2a0-4ff1-a221-7244e9d214c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "######################### Import des données #########################\n",
        "\n",
        "path = \"/content/drive/My Drive/D4G/\"\n",
        "\n",
        "#for testing\n",
        "\"\"\"MSI = ['1_MSI_Seredou_2015_tiled/S2A_20151203_MSI_1km2_1.tif']\n",
        "test = ['1_Images_Seredou_2015_16bit_tiled/S2A_20151203_seredou_1km2_1.tif']\n",
        "truth = ['1_Mask_Seredou_2015_tiled/GroundTruth_seredou_20151203_1km2_1.tif']\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"MSI = ['1_MSI_Seredou_2015_tiled/S2A_20151203_MSI_1km2_1.tif']\\ntest = ['1_Images_Seredou_2015_16bit_tiled/S2A_20151203_seredou_1km2_1.tif']\\ntruth = ['1_Mask_Seredou_2015_tiled/GroundTruth_seredou_20151203_1km2_1.tif']\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sta9I5a2KZcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader, sampler\n",
        "\n",
        "class OurDataset_train(Dataset):\n",
        "    \"\"\"This dataset includes .... \"\"\"\n",
        "    \n",
        "    def __init__(self, path, idx_train, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            path: (str) path to the images directory.\n",
        "        \"\"\"\n",
        "        #for prod\n",
        "        test_raw = os.listdir(path + \"Images\")\n",
        "        MSI_raw = os.listdir(path + \"MSI\")\n",
        "        truth_raw = os.listdir(path + \"Maskm\")\n",
        "\n",
        "        self.idx = idx_train\n",
        "\n",
        "        self.test = {i:('Images/'+test_raw[i]) for i in idx_train}\n",
        "        self.MSI = {i:('MSI/'+MSI_raw[i]) for i in idx_train}\n",
        "        self.mask = {i:('Maskm/'+truth_raw[i]) for i in idx_train}\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.idx)\n",
        "\n",
        "    def __getitem__(self, el):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx: (int) the index of the subject/session whom data is loaded.\n",
        "        Returns:\n",
        "            sample: (dict) corresponding data described by the following keys:\n",
        "                scan: \n",
        "                mask: \n",
        "                PatientID: \n",
        "                SourceDataset\t\n",
        "        \"\"\"\n",
        "        #Read the rasters as array\n",
        "        ds1,featuretest_MSI = raster.read(path + self.MSI[self.idx[el]], bands='all')\n",
        "        ds2,featuretest = raster.read(path + self.test[self.idx[el]], bands='all')\n",
        "        ds3,truth = raster.read(path + self.mask[self.idx[el]], bands='all')\n",
        "        \n",
        "        #On remplace les valeurs négatives par 0\n",
        "        featuretest_MSI[featuretest_MSI < 0] = 0\n",
        "        featuretest[featuretest < 0] = 0\n",
        "        featuretest = np.append(featuretest, np.array([featuretest_MSI]),axis=0)\n",
        "        \n",
        "        preprocessed_truth = np.array([changeDimension(truth)])\n",
        "        \n",
        "        #On importe les données\n",
        "        for i in range (11):\n",
        "        \n",
        "            featuretest[i] = featuretest[i]/featuretest[i].max()\n",
        "            \n",
        "            if i == 0:\n",
        "                preprocessed_data = np.array([changeDimension(featuretest[i])])\n",
        "            else:\n",
        "                preprocessed_data = np.append(preprocessed_data, [changeDimension(featuretest[i])], axis = 0)\n",
        "            \n",
        "            #On calcule les moyennes des cases adjacentes - creation de 11 nouvelles colonnes\n",
        "            #featuretest_avg = scipy.signal.convolve2d(featuretest[i],np.ones((5,5)),'same')/25\n",
        "            #Pour un filtre médian\n",
        "            preprocessed_data = np.append(preprocessed_data, [changeDimension(scipy.signal.medfilt(featuretest[i],[25,25]))], axis = 0)\n",
        "            preprocessed_data = np.append(preprocessed_data, [changeDimension(scipy.signal.medfilt(featuretest[i],[11,11]))], axis = 0)\n",
        "            #preprocessed_data = np.append(preprocessed_data, [changeDimension(scipy.signal.medfilt(featuretest[i],[5,5]))], axis = 0)\n",
        "            #preprocessed_data = np.append(preprocessed_data, [changeDimension(scipy.signal.medfilt(featuretest[i],[3,3]))], axis = 0)\n",
        "            #preprocessed_data = np.append(preprocessed_data, [changeDimension(scipy.ndimage.gaussian_filter(featuretest[i],1))], axis = 0)\n",
        "            preprocessed_data = np.append(preprocessed_data, [changeDimension(scipy.ndimage.gaussian_laplace(featuretest[i],1))], axis = 0)\n",
        "            preprocessed_data = np.append(preprocessed_data, [changeDimension(scipy.ndimage.sobel(featuretest[i],1))], axis = 0)\n",
        "        \n",
        "        data = torch.from_numpy(np.transpose(preprocessed_data)).type(torch.FloatTensor)\n",
        "        mask = torch.from_numpy(preprocessed_truth).type(torch.LongTensor)\n",
        "        sample = {'data': data, 'mask': torch.squeeze(mask)}\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Put all the transforms of the dataset in training mode\"\"\"\n",
        "        self.transform.train()\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"Put all the transforms of the dataset in evaluation mode\"\"\"\n",
        "        self.transform.eval()\n",
        "\n",
        "class OurDataset_test(Dataset):\n",
        "    \"\"\"This dataset includes .... \"\"\"\n",
        "    \n",
        "       \n",
        "    def __init__(self, path, idx_test, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            path: (str) path to the images directory.\n",
        "        \"\"\"\n",
        "        #for prod\n",
        "        test_raw = os.listdir(path + \"Images\")\n",
        "        MSI_raw = os.listdir(path + \"MSI\")\n",
        "\n",
        "        self.idx = idx_test\n",
        "\n",
        "        self.test = {i:('Images/'+test_raw[i]) for i in idx_test}\n",
        "        self.MSI = {i:('MSI/'+MSI_raw[i]) for i in idx_test}\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.idx)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx: (int) the index of the subject/session whom data is loaded.\n",
        "        Returns:\n",
        "            sample: (dict) corresponding data described by the following keys:\n",
        "                scan: \n",
        "                mask: \n",
        "                PatientID: \n",
        "                SourceDataset\t\n",
        "        \"\"\"\n",
        "        #Read the rasters as array\n",
        "        ds1,featuretest_MSI = raster.read(path + MSI[self.idx[i]], bands='all')\n",
        "        ds2,featuretest = raster.read(path + test[self.idx[i]], bands='all')\n",
        "\n",
        "        #On remplace les valeurs négatives par 0\n",
        "        featuretest_MSI[featuretest_MSI < 0] = 0\n",
        "        featuretest[featuretest < 0] = 0\n",
        "        featuretest = np.append(featuretest, np.array([featuretest_MSI]),axis=0)\n",
        "\n",
        "        #On importe les données\n",
        "        for i in range (11):\n",
        "        \n",
        "            featuretest[i] = featuretest[i]/featuretest[i].max()\n",
        "            \n",
        "            if i == 0:\n",
        "                preprocessed_data = np.array([changeDimension(featuretest[i])])\n",
        "            else:\n",
        "                preprocessed_data = np.append(preprocessed_data, [changeDimension(featuretest[i])], axis = 0)\n",
        "            \n",
        "            #On calcule les moyennes des cases adjacentes - creation de 11 nouvelles colonnes\n",
        "            #featuretest_avg = scipy.signal.convolve2d(featuretest[i],np.ones((5,5)),'same')/25\n",
        "            #Pour un filtre médian\n",
        "            preprocessed_data = np.append(preprocessed_data, [changeDimension(scipy.signal.medfilt(featuretest[i],[25,25]))], axis = 0)\n",
        "            preprocessed_data = np.append(preprocessed_data, [changeDimension(scipy.signal.medfilt(featuretest[i],[11,11]))], axis = 0)\n",
        "            #preprocessed_data = np.append(preprocessed_data, [changeDimension(scipy.signal.medfilt(featuretest[i],[5,5]))], axis = 0)\n",
        "            #preprocessed_data = np.append(preprocessed_data, [changeDimension(scipy.signal.medfilt(featuretest[i],[3,3]))], axis = 0)\n",
        "            #preprocessed_data = np.append(preprocessed_data, [changeDimension(scipy.ndimage.gaussian_filter(featuretest[i],1))], axis = 0)\n",
        "            preprocessed_data = np.append(preprocessed_data, [changeDimension(scipy.ndimage.gaussian_laplace(featuretest[i],1))], axis = 0)\n",
        "            preprocessed_data = np.append(preprocessed_data, [changeDimension(scipy.ndimage.sobel(featuretest[i],1))], axis = 0)\n",
        "        \n",
        "        data = torch.from_numpy(np.transpose(preprocessed_data)).type(torch.FloatTensor)\n",
        "        sample = {'data': data}\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Put all the transforms of the dataset in training mode\"\"\"\n",
        "        self.transform.train()\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"Put all the transforms of the dataset in evaluation mode\"\"\"\n",
        "        self.transform.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG7nihqFQEq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, criterion, optimizer, n_epochs):\n",
        "    \"\"\"\n",
        "    Method used to train a nn\n",
        "    \n",
        "    Args:\n",
        "        model: (nn.Module) the neural network\n",
        "        train_loader: (DataLoader) a DataLoader wrapping the dataset\n",
        "        criterion: (nn.Module) a method to compute the loss of a mini-batch of images\n",
        "        optimizer: (torch.optim) an optimization algorithm\n",
        "        n_epochs: (int) number of epochs performed during training\n",
        "\n",
        "    Returns:\n",
        "        best_model: (nn.Module) the trained neural network\n",
        "    \"\"\"\n",
        "    best_model = deepcopy(model)\n",
        "    train_best_loss = np.inf\n",
        "\n",
        "    batch_size = train_loader.batch_size\n",
        "    n = 10\n",
        "\n",
        "    n_batches = n//batch_size\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for i, data in enumerate(train_loader):\n",
        "            images, mask = data['data'], data['mask']\n",
        "            scans = torch.flatten(mask)\n",
        "            outputs = torch.reshape(model(images), (scans.shape[0],4))\n",
        "            loss = criterion(outputs, scans)\n",
        "            loss.backward()\n",
        "            total_loss += loss.item()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            print(\"Batch {}/{} loss : {} accuracy : {}\".format(i+1, n_batches, str(total_loss/((i+1)*batch_size)),str(int(torch.sum(torch.argmax(outputs,1) == scans))/scans.shape[0])))\n",
        "\n",
        "        mean_loss = total_loss / len(train_loader.dataset)\n",
        "        print('Epoch %i: loss = %f' % (epoch, mean_loss))\n",
        "\n",
        "        if mean_loss < train_best_loss:\n",
        "            best_model = deepcopy(model)\n",
        "            train_best_loss = mean_loss\n",
        "    \n",
        "    return best_model\n",
        "\n",
        "def test(model, data_loader, criterion):\n",
        "    \"\"\"\n",
        "    Method used to test a CNN\n",
        "    \n",
        "    Args:\n",
        "        model: (nn.Module) the neural network\n",
        "        data_loader: (DataLoader) a DataLoader wrapping a MRIDataset\n",
        "        criterion: (nn.Module) a method to compute the loss of a mini-batch of images\n",
        "    \n",
        "    Returns:\n",
        "        results_df: (DataFrame) the label predicted for on the slice level.\n",
        "        results_metrics: (dict) a set of metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    columns = [\"participant_id\", \"slice_id\", \"proba0\", \"proba1\",\n",
        "               \"true_label\", \"predicted_label\"]\n",
        "    results_df = pd.DataFrame(columns=columns)\n",
        "    total_loss = 0\n",
        "\n",
        "    batch_size = data_loader.batch_size\n",
        "    n = 10000\n",
        "\n",
        "    n_batches = n//batch_size\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(data_loader, 0):\n",
        "             images, mask = data['data'], data['mask']\n",
        "             scans = torch.flatten(mask)\n",
        "             outputs = torch.reshape(model(images), (scans.shape[0],4))\n",
        "             loss = criterion(outputs, scans)\n",
        "             total_loss += loss.item()\n",
        "             print(\"Batch {}/{} loss : {}\".format(i+1, n_batches, str(total_loss/((i+1)*batch_size))))\n",
        "       #     probs = nn.Softmax(dim=1)(outputs)\n",
        "        #    _, predicted = torch.max(outputs.data, 1)\n",
        "             print(\"Accuracy : {}\".format(str(int(torch.sum(torch.argmax(outputs,1) == scans))/scans.shape[0])))\n",
        "\n",
        "    print(\"Final loss : {}\".format(str(total_loss/ len(data_loader.dataset))))\n",
        "           \n",
        "    return 0\n",
        "\n",
        "\n",
        "def compute_metrics(ground_truth, prediction):\n",
        "    \"\"\"Computes the accuracy, sensitivity, specificity and balanced accuracy\"\"\"\n",
        "    metrics_dict = dict()\n",
        "    metrics_dict['cMatrix'] = confusion_matrix(ground_truth, prediction)\n",
        "    metrics_dict['pscore'] = precision_score(ground_truth, prediction,average='micro')\n",
        "    metrics_dict['rscore'] = recall_score(ground_truth, prediction,average='micro')\n",
        "    \n",
        "    return metrics_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krUZkF84UOz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################## Modèle ############################\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.lin1 = nn.Linear(55, 200)\n",
        "        self.lin2 = nn.Linear(200, 200)\n",
        "        self.lin3 = nn.Linear(200, 200)\n",
        "        self.lin4 = nn.Linear(200, 4)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.lin2(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.lin3(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.lin4(x)\n",
        "        return self.relu(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEVKRJieJtJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################## Prétraitement ############################\n",
        "\n",
        "batch_size = 1000\n",
        "\n",
        "#Decoupage en train test \n",
        "idx = [i for i in range(len(os.listdir(path + \"MSI\")))]\n",
        "idx_train, idx_val= train_test_split(idx, test_size = 0.2, random_state=42, shuffle=True)\n",
        "\n",
        "database_train = OurDataset_train(path, idx_train)\n",
        "dataloader_train = DataLoader(database_train, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "model = Model()\n",
        "criterion = nn.MultiMarginLoss(p=2)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "n_epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsztqK9WQfRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################### Entrainement du modèle ##########################\n",
        "\n",
        "train(model, dataloader_train, criterion, optimizer, n_epochs)\n",
        "torch.save(model.state_dict(), \"model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-4hoPV4KJ20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################### Statistiques ################################\n",
        "\n",
        "#Predict for test data \n",
        "\n",
        "database_val = OurDataset_train(path, idx_val)\n",
        "dataloader_val = DataLoader(database_val, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "results_metrics = test(model, dataloader_val, criterion)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}