{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "D4G LNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3qr0ZB6LUQ4",
        "colab_type": "code",
        "outputId": "21d02f18-efc3-4e2c-9674-8681becc5a77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBZbfd5FTk5j",
        "colab_type": "code",
        "outputId": "fb2c058b-b843-41b6-fad8-7d524a6c629e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install pyrsgis"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyrsgis in /usr/local/lib/python3.6/dist-packages (0.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idlZ5PZxJi8H",
        "colab_type": "code",
        "outputId": "a87d8506-d5b5-4ed3-b309-48f251b1bdc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "######################### Import des libraries #######################\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader, sampler\n",
        "from pyrsgis import raster\n",
        "from pyrsgis.convert import changeDimension\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import scipy.signal\n",
        "import scipy.ndimage\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning! matplotlib_scalebar library not found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N830fS93Jof2",
        "colab_type": "code",
        "outputId": "1ac27aed-1a02-49f0-f7ad-a49711a0a472",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "######################### Import des données (1 seule tile) #########################\n",
        "\n",
        "path = \"/content/drive/My Drive/D4G/\"\n",
        "\n",
        "Test = \"2_Image_Seredou_32bits_20170205/S2A_20170205_seredou_32bits.tif\"\n",
        "MSI = \"2_MSI_Seredou_20170205/S2A_20170205_seredou_ZE_MSI_89.tif\"\n",
        "Truth = \"GroundTruth_Seredou_20170205/GroundTruth_20170205_seredou_ZE_89.tif\"\n",
        "\"\"\"\n",
        "path = \"/content/drive/My Drive/D4G/4_Congo_20200128/\"\n",
        "\n",
        "Test = \"Image_Congo_20200128.tif\"\n",
        "MSI = \"MSI_Congo_20200128.tif\"\n",
        "Truth = \"GroundTruth_Congo_20200128.tif\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npath = \"/content/drive/My Drive/D4G/4_Congo_20200128/\"\\n\\nTest = \"Image_Congo_20200128.tif\"\\nMSI = \"MSI_Congo_20200128.tif\"\\nTruth = \"GroundTruth_Congo_20200128.tif'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1AyKJcFLdbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################### Import des données (tout en 1 seule tile) #########################\n",
        "\n",
        "path = \"/content/drive/My Drive/D4G/Seredou_20190307_v2.tif\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQo7k4X1njZR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################### Import des données (plusieurs tiles) #########################\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/D4G/1_Seredou_2015/\")\n",
        "\n",
        "#for testing\n",
        "\"\"\"MSI = ['1_MSI_Seredou_2015_tiled\\\\S2A_20151203_MSI_1km2_1.tif']\n",
        "test = ['1_Images_Seredou_2015_16bit_tiled\\\\S2A_20151203_seredou_1km2_1.tif']\n",
        "truth = ['1_Mask_Seredou_2015_tiled\\\\GroundTruth_seredou_20151203_1km2_1.tif']\"\"\"\n",
        "\n",
        "#for prod\n",
        "test_raw = os.listdir(\"1_Images_Seredou_2015_32bits\")\n",
        "MSI_raw = os.listdir(\"1_MSI_Seredou_2015\")\n",
        "truth_raw = os.listdir(\"1_GroundTruth_Seredou_2015_tiled\")\n",
        "\n",
        "Test = {int(k.split(\"_\")[-1].split(\".tif\")[0]):('1_Images_Seredou_2015_32bits/'+k) for k in test_raw}\n",
        "MSI = {int(k.split(\"_\")[-1].split(\".tif\")[0]):('1_MSI_Seredou_2015/'+k) for k in MSI_raw}\n",
        "Truth = {int(k.split(\"_\")[-1].split(\".tif\")[0].split(\"(\")[0]):('1_GroundTruth_Seredou_2015_tiled/'+k) for k in truth_raw}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiKGhhz2GDRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################### Fonctions auxiliaires #########################\n",
        "\n",
        "def divide_test_in_squares(size, img):\n",
        "    return [img[:,int(i % img.shape[0])*size:((int(i % img.shape[0])+1)*size), int(i // img.shape[0])*size:(int(i // img.shape[0])+1)*size] for i in range(int(img.shape[0] * img.shape[1] // size**2))]\n",
        "\n",
        "def divide_image_in_squares(size, img):\n",
        "    return [img[int(i % img.shape[0])*size:((int(i % img.shape[0])+1)*size), int(i // img.shape[0])*size:(int(i // img.shape[0])+1)*size] for i in range(int(img.shape[0] * img.shape[1] // size**2))]\n",
        "\n",
        "def get_window(img, center, size):\n",
        "    \n",
        "    window = torch.tensor([0. for i in range(size*size)])\n",
        "    if center[0] + size // 2 < img.shape[0]:\n",
        "        i_begin = max(0,int(center[0]-size//2))\n",
        "        i_end = i_begin + size\n",
        "    else:\n",
        "        i_end = img.shape[0]\n",
        "        i_begin = i_end - size\n",
        "    if center[1] + size // 2 < img.shape[1]:\n",
        "        j_begin = max(0,int(center[1]-size//2))\n",
        "        j_end = j_begin + size\n",
        "    else:\n",
        "        j_end = img.shape[1]\n",
        "        j_begin = j_end - size\n",
        "\n",
        "    for i in range(i_begin, i_end):\n",
        "        for j in range(j_begin, j_end):\n",
        "            window[i - i_begin + size*(j - j_begin)] = float(img[i,j])\n",
        "    return window"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sta9I5a2KZcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################### Base de données (1 image) #########################\n",
        "\n",
        "class OurDataset(Dataset):\n",
        "    \"\"\"This dataset includes .... \"\"\"\n",
        "    \n",
        "    def __init__(self, path, Test, MSI, Truth, size, percentage, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            path: (str) path to the images directory.\n",
        "        \"\"\"\n",
        "        #get data\n",
        "        ds1, data_MSI = raster.read(path + MSI, bands='all')\n",
        "        ds2, data_test = raster.read(path + Test, bands='all')\n",
        "        ds3, data_truth = raster.read(path + Truth, bands='all')\n",
        "\n",
        "        data_MSI[data_MSI < 0] = 0\n",
        "        data_test[data_test < 0] = 0\n",
        "        data_truth[data_truth < 0] = 0\n",
        "\n",
        "        self.MSI = divide_image_in_squares(size, data_MSI)\n",
        "        self.Test = divide_test_in_squares(size, data_test)\n",
        "        self.Truth = divide_image_in_squares(size, data_truth)\n",
        "  \n",
        "        self.size = size\n",
        "        self.mode = \"train\"\n",
        "        self.id_train, self.id_test = train_test_split([i for i in range(len(self.Test))], test_size = percentage, random_state=42, shuffle=True)\n",
        "        \n",
        "    def __len__(self):\n",
        "        if self.mode == \"train\":\n",
        "            return len(self.id_train)\n",
        "        else:\n",
        "            return len(self.id_test)\n",
        "\n",
        "    def __getitem__(self, id):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx: (int) the index of the subject/session whom data is loaded.\n",
        "        Returns:\n",
        "            sample: (dict) corresponding data described by the following keys:\n",
        "                scan: 11 channels value\n",
        "                mask: true value\n",
        "        \"\"\"\n",
        "        if self.mode == \"train\":\n",
        "            idx = self.id_train[id]\n",
        "        else:\n",
        "            idx = self.id_test[id]\n",
        "\n",
        "        #get data\n",
        "        MSI = self.MSI[idx]\n",
        "        Test = self.Test[idx]\n",
        "        Truth = self.Truth[idx]\n",
        "\n",
        "        feature_data = torch.tensor([[0. for i in range(11*10)] for j in range(Test.shape[1] * Test.shape[2])])\n",
        "        feature_truth = np.array([0 for j in range(Test.shape[1] * Test.shape[2])])\n",
        "\n",
        "        #On importe les données\n",
        "        for i in range(Test.shape[1]):\n",
        "            for j in range(Test.shape[2]):\n",
        "                feature_truth[i+Test.shape[1]*j] = Truth[i,j]\n",
        "                \n",
        "                msi = get_window(MSI, (i,j), self.size // 2)\n",
        "                if msi.max() != 0:\n",
        "                    msi = msi/msi.max()\n",
        "                laplace_gauss = torch.tensor(scipy.ndimage.gaussian_laplace(msi.numpy(), sigma = 1))\n",
        "\n",
        "                feature_data[i,j,100] = float(MSI[i,j])\n",
        "                feature_data[i,j,101] = msi.max()\n",
        "                feature_data[i,j,102] = msi.min()\n",
        "                feature_data[i,j,103] = msi.mean()\n",
        "                feature_data[i,j,104] = msi.std()\n",
        "                feature_data[i,j,105] = msi.median()\n",
        "                feature_data[i,j,106] = laplace_gauss.min()\n",
        "                feature_data[i,j,107] = laplace_gauss.max()\n",
        "                feature_data[i,j,108] = laplace_gauss.mean()\n",
        "                feature_data[i,j,109] = laplace_gauss.std()\n",
        "                \n",
        "                for k in range (11):\n",
        "                    test = get_window(Test[k,:,:], (i,j), 5)\n",
        "                    if test.max() != 0:\n",
        "                        test = test/test.max()\n",
        "\n",
        "                    laplace_gauss = torch.tensor(scipy.ndimage.gaussian_laplace(get_window(Test[k,:,:], (i,j), 11).numpy(), sigma = 1))\n",
        "\n",
        "                    feature_data[i+Test.shape[1]*j,k*10] = float(Test[k,i,j])\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 1] = test.max()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 2] = test.min()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 3] = test.mean()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 4] = test.std()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 5] = test.median()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 6] = laplace_gauss.max()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 7] = laplace_gauss.min()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 8] = laplace_gauss.mean()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 9] = laplace_gauss.std()\n",
        "         \n",
        "        sample = {'data': feature_data, 'mask': feature_truth}\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Put all the transforms of the dataset in training mode\"\"\"\n",
        "        self.transform.train()\n",
        "\n",
        "    def set_mode(self, mode):\n",
        "        \"\"\"Change mode of the database\"\"\"\n",
        "        self.mode = mode\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"Put all the transforms of the dataset in evaluation mode\"\"\"\n",
        "        self.transform.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmPeuGUILUiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################### Base de données (tout en 1 image) #########################\n",
        "\n",
        "class OurDataset(Dataset):\n",
        "    \"\"\"This dataset includes .... \"\"\"\n",
        "    \n",
        "    def __init__(self, path, size, percentage, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            path: (str) path to the images directory.\n",
        "        \"\"\"\n",
        "        #get data\n",
        "        ds1, data = raster.read(path, bands='all')\n",
        "\n",
        "        data[data < 0] = 0\n",
        "\n",
        "        self.Test = divide_test_in_squares(size, data[:11,:,:])\n",
        "        self.Truth = divide_image_in_squares(size, data[11,:,:])\n",
        "        self.size = size\n",
        "        self.mode = \"train\"\n",
        "        self.id_train, self.id_test = train_test_split([i for i in range(len(self.Test))], test_size = percentage, random_state=42, shuffle=True)\n",
        "        \n",
        "    def __len__(self):\n",
        "        if self.mode == \"train\":\n",
        "            return len(self.id_train)\n",
        "        else:\n",
        "            return len(self.id_test)\n",
        "\n",
        "    def __getitem__(self, id):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx: (int) the index of the subject/session whom data is loaded.\n",
        "        Returns:\n",
        "            sample: (dict) corresponding data described by the following keys:\n",
        "                scan: 11 channels value\n",
        "                mask: true value\n",
        "        \"\"\"\n",
        "        if self.mode == \"train\":\n",
        "            idx = self.id_train[id]\n",
        "        else:\n",
        "            idx = self.id_test[id]\n",
        "\n",
        "        Test = self.Test[idx]\n",
        "        Truth = self.Truth[idx]\n",
        "\n",
        "        feature_data = torch.tensor([[0. for i in range(11*10)] for j in range(Test.shape[1] * Test.shape[2])])\n",
        "        feature_truth = np.array([0 for j in range(Test.shape[1] * Test.shape[2])])\n",
        "\n",
        "        #On importe les données\n",
        "        for i in range(Test.shape[1]):\n",
        "            for j in range(Test.shape[2]):\n",
        "                feature_truth[i+Test.shape[1]*j] = Truth[i,j]\n",
        "                \n",
        "                for k in range (11):\n",
        "                    test = get_window(Test[k,:,:], (i,j), 5)\n",
        "                    if test.max() != 0:\n",
        "                        test = test/test.max()\n",
        "\n",
        "                    laplace_gauss = torch.tensor(scipy.ndimage.gaussian_laplace(get_window(Test[k,:,:], (i,j), 11).numpy(), sigma = 1))\n",
        "\n",
        "                    feature_data[i+Test.shape[1]*j,k*10] = float(Test[k,i,j])\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 1] = test.max()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 2] = test.min()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 3] = test.mean()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 4] = test.std()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 5] = test.median()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 6] = laplace_gauss.max()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 7] = laplace_gauss.min()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 8] = laplace_gauss.mean()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 9] = laplace_gauss.std()\n",
        "         \n",
        "        sample = {'data': feature_data, 'mask': feature_truth}\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Put all the transforms of the dataset in training mode\"\"\"\n",
        "        self.transform.train()\n",
        "\n",
        "    def set_mode(self, mode):\n",
        "        \"\"\"Change mode of the database\"\"\"\n",
        "        self.mode = mode\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"Put all the transforms of the dataset in evaluation mode\"\"\"\n",
        "        self.transform.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32zCY29et_uY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################### Base de données (plusieurs images) #########################\n",
        "\n",
        "class OurDataset(Dataset):\n",
        "    \"\"\"This dataset includes .... \"\"\"\n",
        "    \n",
        "    def __init__(self, path, Test, MSI, Truth, percentage, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            path: (str) path to the images directory.\n",
        "        \"\"\"\n",
        "        self.MSI = {}\n",
        "        self.Test = {}\n",
        "        self.Truth = {}\n",
        "        for key in MSI.keys():\n",
        "            _, self.MSI[key] = raster.read(MSI[key], bands='all')\n",
        "            _, self.Test[key] = raster.read(Test[key], bands='all')\n",
        "            _, self.Truth[key] = raster.read(Truth[key], bands='all')\n",
        "\n",
        "        self.mode = \"train\"\n",
        "        self.id_train, self.id_test = train_test_split([i for i in range(len(list(self.Test.keys())))], test_size = percentage, random_state=42, shuffle=True)\n",
        "        \n",
        "    def __len__(self):\n",
        "        if self.mode == \"train\":\n",
        "            return len(self.id_train)\n",
        "        else:\n",
        "            return len(self.id_test)\n",
        "\n",
        "    def __getitem__(self, id):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx: (int) the index of the subject/session whom data is loaded.\n",
        "        Returns:\n",
        "            sample: (dict) corresponding data described by the following keys:\n",
        "                scan: 11 channels value\n",
        "                mask: true value\n",
        "        \"\"\"\n",
        "\n",
        "        if self.mode == \"train\":\n",
        "            idx = self.id_train[id] + 1\n",
        "        else:\n",
        "            idx = self.id_test[id] + 1\n",
        "\n",
        "        #get data\n",
        "        MSI = self.MSI[idx]\n",
        "        Test = self.Test[idx]\n",
        "        Truth = self.Truth[idx]\n",
        "        \n",
        "        MSI[MSI < 0] = 0\n",
        "        Test[Test < 0] = 0\n",
        "        Truth[Truth < 0] = 0\n",
        "        \n",
        "        feature_data = torch.tensor([[0. for i in range(11*10)] for j in range(Test.shape[1] * Test.shape[2])])\n",
        "        feature_truth = np.array([0 for j in range(Test.shape[1] * Test.shape[2])])\n",
        "\n",
        "        #On importe les données\n",
        "        for i in range(Test.shape[1]):\n",
        "            for j in range(Test.shape[2]):\n",
        "                feature_truth[i+Test.shape[1]*j] = Truth[i,j]\n",
        "                \n",
        "                msi = get_window(MSI, (i,j), self.size // 2)\n",
        "                if msi.max() != 0:\n",
        "                    msi = msi/msi.max()\n",
        "                laplace_gauss = torch.tensor(scipy.ndimage.gaussian_laplace(msi.numpy(), sigma = 1))\n",
        "\n",
        "                feature_data[i,j,100] = float(MSI[i,j])\n",
        "                feature_data[i,j,101] = msi.max()\n",
        "                feature_data[i,j,102] = msi.min()\n",
        "                feature_data[i,j,103] = msi.mean()\n",
        "                feature_data[i,j,104] = msi.std()\n",
        "                feature_data[i,j,105] = msi.median()\n",
        "                feature_data[i,j,106] = laplace_gauss.min()\n",
        "                feature_data[i,j,107] = laplace_gauss.max()\n",
        "                feature_data[i,j,108] = laplace_gauss.mean()\n",
        "                feature_data[i,j,109] = laplace_gauss.std()\n",
        "                \n",
        "                for k in range (11):\n",
        "                    test = get_window(Test[k,:,:], (i,j), 5)\n",
        "                    if test.max() != 0:\n",
        "                        test = test/test.max()\n",
        "\n",
        "                    laplace_gauss = torch.tensor(scipy.ndimage.gaussian_laplace(get_window(Test[k,:,:], (i,j), 11).numpy(), sigma = 1))\n",
        "\n",
        "                    feature_data[i+Test.shape[1]*j,k*10] = float(Test[k,i,j])\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 1] = test.max()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 2] = test.min()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 3] = test.mean()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 4] = test.std()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 5] = test.median()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 6] = laplace_gauss.max()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 7] = laplace_gauss.min()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 8] = laplace_gauss.mean()\n",
        "                    feature_data[i+Test.shape[1]*j,k*10 + 9] = laplace_gauss.std()\n",
        "         \n",
        "        sample = {'data': feature_data, 'mask': feature_truth}\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Put all the transforms of the dataset in training mode\"\"\"\n",
        "        self.transform.train()\n",
        "\n",
        "    def set_mode(self, mode):\n",
        "        \"\"\"Change mode of the database\"\"\"\n",
        "        self.mode = mode\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"Put all the transforms of the dataset in evaluation mode\"\"\"\n",
        "        self.transform.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG7nihqFQEq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, criterion, optimizer, n_epochs, size):\n",
        "    \"\"\"\n",
        "    Method used to train the LNN\n",
        "    \n",
        "    Args:\n",
        "        model: (nn.Module) the neural network\n",
        "        train_loader: (DataLoader) a DataLoader wrapping the dataset\n",
        "        criterion: (nn.Module) a method to compute the loss of a mini-batch of images\n",
        "        optimizer: (torch.optim) an optimization algorithm\n",
        "        n_epochs: (int) number of epochs performed during training\n",
        "\n",
        "    Returns:\n",
        "        best_model: (nn.Module) the trained neural network\n",
        "    \"\"\"\n",
        "    best_model = deepcopy(model)\n",
        "    train_best_loss = np.inf\n",
        "\n",
        "    batch_size = train_loader.batch_size\n",
        "    n = 10\n",
        "\n",
        "    n_batches = n//batch_size\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        total_eq = 0\n",
        "        total_nb = 0\n",
        "\n",
        "        for i, data in enumerate(train_loader):\n",
        "            images, mask = torch.reshape(data['data'], ((size**2)*batch_size,110)), data['mask']\n",
        "            scans = torch.flatten(mask) - 1\n",
        "            outputs = torch.reshape(model(images), (scans.shape[0],3))\n",
        "            loss = criterion(outputs, scans)\n",
        "            loss.backward()\n",
        "            soft_outputs = nn.functional.softmax(outputs)\n",
        "            total_eq += int(torch.sum(torch.argmax(soft_outputs,1) == scans))\n",
        "            total_loss += loss.item()\n",
        "            total_nb += scans.shape[0]\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "        mean_loss = total_loss / len(train_loader.dataset)\n",
        "        print('Epoch %i: loss = %f & accuracy = %f' % (epoch, mean_loss, total_eq/total_nb))\n",
        "\n",
        "        if mean_loss < train_best_loss:\n",
        "            best_model = deepcopy(model)\n",
        "            train_best_loss = mean_loss\n",
        "    \n",
        "    return best_model\n",
        "\n",
        "def test(model, data_loader, criterion, size):\n",
        "    \"\"\"\n",
        "    Method used to test a LNN\n",
        "    \n",
        "    Args:\n",
        "        model: (nn.Module) the neural network\n",
        "        data_loader: (DataLoader) a DataLoader wrapping the dataset\n",
        "        criterion: (nn.Module) a method to compute the loss of a mini-batch of images\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    batch_size = data_loader.batch_size\n",
        "    n = 10000\n",
        "\n",
        "    n_batches = n//batch_size\n",
        "    nb_true = 0\n",
        "    nb_total = 0\n",
        "    size_loss = 0\n",
        "    total_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(data_loader):\n",
        "            images, mask = torch.reshape(data['data'], ((size**2)*batch_size,110)), data['mask']\n",
        "            scans = torch.flatten(mask) - 1\n",
        "            outputs = torch.reshape(model(images), (scans.shape[0],3))\n",
        "            loss = criterion(outputs, scans)\n",
        "            soft_outputs = nn.functional.softmax(outputs)\n",
        "            total_loss += loss.item()\n",
        "            size_loss += batch_size\n",
        "            nb_true += int(torch.sum(torch.argmax(outputs,1) == scans))\n",
        "            nb_total += scans.shape[0]\n",
        "\n",
        "    print(\"Final loss : {} & accuracy : {}\".format(str(total_loss/ size_loss), str(nb_true/nb_total)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krUZkF84UOz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################## Modèle ############################\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.lin1 = nn.Linear(110, 110)\n",
        "        self.lin2 = nn.Linear(110, 110)\n",
        "        self.lin3 = nn.Linear(110, 110)\n",
        "        self.lin4 = nn.Linear(110, 110)\n",
        "        self.lin5 = nn.Linear(110, 3)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.lin2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.lin3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.lin4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.lin5(x)\n",
        "        return nn.functional.softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg7aN91AKbgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################## Database ############################\n",
        "\n",
        "size = 20\n",
        "#database = OurDataset(path, Test, MSI, Truth, 0.2)\n",
        "database = OurDataset(path, size, 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEVKRJieJtJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################## Prétraitement ############################\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "database.set_mode(\"train\")\n",
        "dataloader_train = DataLoader(database, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "model = Model()\n",
        "criterion = nn.MultiMarginLoss(p=1)\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.07)\n",
        "n_epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsztqK9WQfRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################### Entrainement du modèle ##########################\n",
        "\n",
        "train(model, dataloader_train, criterion, optimizer, n_epochs, size)\n",
        "torch.save(model.state_dict(), \"model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-4hoPV4KJ20",
        "colab_type": "code",
        "outputId": "7e3e47e0-31b0-4d21-c49b-8596cdfe81a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "############################### Statistiques ################################\n",
        "\n",
        "#Predict for test data \n",
        "\n",
        "database.set_mode(\"test\")\n",
        "dataloader_val = DataLoader(database, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "test(model, dataloader_val, criterion, size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final loss : 0.05909258921941121 & accuracy : 0.4681666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}