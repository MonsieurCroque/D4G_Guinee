{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "D4G cnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3qr0ZB6LUQ4",
        "colab_type": "code",
        "outputId": "90745fb1-d40c-48b8-9916-7d8fe8943015",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBZbfd5FTk5j",
        "colab_type": "code",
        "outputId": "28e3fe73-2068-4fd2-bf4a-9c39784a56ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install pyrsgis"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyrsgis in /usr/local/lib/python3.6/dist-packages (0.3.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idlZ5PZxJi8H",
        "colab_type": "code",
        "outputId": "f619ea1d-7a1b-4bff-8c00-a9207a3bc801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "######################### Import des libraries #######################\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader, sampler\n",
        "from pyrsgis import raster\n",
        "from pyrsgis.convert import changeDimension\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import scipy.signal\n",
        "import scipy.ndimage\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning! matplotlib_scalebar library not found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44yhFLpUJjdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################## Parameters ############################\n",
        "\n",
        "stop = 1000\n",
        "epochs = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N830fS93Jof2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################### Import des données #########################\n",
        "\n",
        "path = \"/content/drive/My Drive/D4G/2_Seredou_2017/\"\n",
        "\n",
        "Test = \"2_Image_Seredou_32bits_20170205/S2A_20170205_seredou_32bits.tif\"\n",
        "MSI = \"2_MSI_Seredou_20170205/S2A_20170205_seredou_ZE_MSI_89.tif\"\n",
        "Truth = \"GroundTruth_Seredou_20170205/GroundTruth_20170205_seredou_ZE_89.tif\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiKGhhz2GDRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################### Fonctions auxiliaires #########################\n",
        "\n",
        "def divide_test_in_squares(size, img):\n",
        "    return [img[:,int(i % img.shape[0])*size:((int(i % img.shape[0])+1)*size), int(i // img.shape[0])*size:(int(i // img.shape[0])+1)*size] for i in range(int(img.shape[0] * img.shape[1] // size**2))]\n",
        "\n",
        "def divide_image_in_squares(size, img):\n",
        "    return [img[int(i % img.shape[0])*size:((int(i % img.shape[0])+1)*size), int(i // img.shape[0])*size:(int(i // img.shape[0])+1)*size] for i in range(int(img.shape[0] * img.shape[1] // size**2))]\n",
        "\n",
        "def get_window(img, center, size):\n",
        "    \n",
        "    window = torch.tensor([[0. for i in range(size)] for j in range(size)])\n",
        "    if center[0] + size // 2 < img.shape[0]:\n",
        "        i_begin = max(0,int(center[0]-size//2))\n",
        "        i_end = i_begin + size\n",
        "    else:\n",
        "        i_end = img.shape[0]\n",
        "        i_begin = i_end - size\n",
        "    if center[1] + size // 2 < img.shape[1]:\n",
        "        j_begin = max(0,int(center[1]-size//2))\n",
        "        j_end = j_begin + size\n",
        "    else:\n",
        "        j_end = img.shape[1]\n",
        "        j_begin = j_end - size\n",
        "\n",
        "    for i in range(i_begin, i_end):\n",
        "        for j in range(j_begin, j_end):\n",
        "            window[i - i_begin,j - j_begin] = float(img[i,j])\n",
        "    return window"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sta9I5a2KZcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################### Base de données #########################\n",
        "\n",
        "class OurDataset(Dataset):\n",
        "    \"\"\"This dataset includes .... \"\"\"\n",
        "    \n",
        "    def __init__(self, path, Test, MSI, Truth, size, percentage, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            path: (str) path to the images directory.\n",
        "        \"\"\"\n",
        "        #get data\n",
        "        ds1, data_MSI = raster.read(path + MSI, bands='all')\n",
        "        ds2, data_test = raster.read(path + Test, bands='all')\n",
        "        ds3, data_truth = raster.read(path + Truth, bands='all')\n",
        "\n",
        "        data_MSI[data_MSI < 0] = 0\n",
        "        data_test[data_test < 0] = 0\n",
        "        data_truth[data_truth < 0] = 0\n",
        "\n",
        "        self.MSI = divide_image_in_squares(size, data_MSI)\n",
        "        self.Test = divide_test_in_squares(size, data_test)\n",
        "        self.Truth = divide_image_in_squares(size, data_truth)\n",
        "  \n",
        "        self.size = size\n",
        "        self.mode = \"train\"\n",
        "        self.id_train, self.id_test = train_test_split([i for i in range(len(self.Test))], test_size = percentage, random_state=42, shuffle=True)\n",
        "        \n",
        "    def __len__(self):\n",
        "        if self.mode == \"train\":\n",
        "            return len(self.id_train)\n",
        "        else:\n",
        "            return len(self.id_test)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            idx: (int) the index of the subject/session whom data is loaded.\n",
        "        Returns:\n",
        "            sample: (dict) corresponding data described by the following keys:\n",
        "                scan: 11 channels value\n",
        "                mask: true value\n",
        "        \"\"\"\n",
        "\n",
        "        feature_data = torch.tensor([[[0. for i in range(11*5)] for j in range(self.Test[idx].shape[0])] for k in range(self.Test[idx].shape[1])])\n",
        "        feature_truth = np.array([[0 for j in range(self.Test[idx].shape[0])] for k in range(self.Test[idx].shape[1])])\n",
        "        \n",
        "        #On importe les données\n",
        "        for i in range(self.Test[idx].shape[0]):\n",
        "            for j in range(self.Test[idx].shape[1]):\n",
        "                \n",
        "                feature_truth[i,j] = self.Truth[idx][i,j]\n",
        "\n",
        "                msi = get_window(self.MSI[idx], (i,j), self.size // 2)\n",
        "                msi = msi/msi.max()\n",
        "                feature_data[i,j,50] = float(self.MSI[idx][i,j])\n",
        "                feature_data[i,j,51] = msi.max()\n",
        "                feature_data[i,j,52] = msi.min()\n",
        "                feature_data[i,j,53] = msi.mean()\n",
        "                feature_data[i,j,54] = msi.std()\n",
        "                \n",
        "                for k in range (10):\n",
        "                    test = get_window(self.Test[idx][k,:,:], (i,j), self.size // 2)\n",
        "                    test = test/test.max()\n",
        "                    \n",
        "                    feature_data[i,j,k*5] = float(self.Test[idx][k,i,j])\n",
        "                    feature_data[i,j,k*5 + 1] = test.max()\n",
        "                    feature_data[i,j,k*5 + 2] = test.min()\n",
        "                    feature_data[i,j,k*5 + 3] = test.mean()\n",
        "                    feature_data[i,j,k*5 + 4] = test.std()\n",
        "        \n",
        "        sample = {'data': feature_data, 'mask': feature_truth}\n",
        "\n",
        "        return sample\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Put all the transforms of the dataset in training mode\"\"\"\n",
        "        self.transform.train()\n",
        "\n",
        "    def set_mode(self, mode):\n",
        "        \"\"\"Change mode of the database\"\"\"\n",
        "        self.mode = mode\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"Put all the transforms of the dataset in evaluation mode\"\"\"\n",
        "        self.transform.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG7nihqFQEq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_loader, criterion, optimizer, n_epochs):\n",
        "    \"\"\"\n",
        "    Method used to train a nn\n",
        "    \n",
        "    Args:\n",
        "        model: (nn.Module) the neural network\n",
        "        train_loader: (DataLoader) a DataLoader wrapping the dataset\n",
        "        criterion: (nn.Module) a method to compute the loss of a mini-batch of images\n",
        "        optimizer: (torch.optim) an optimization algorithm\n",
        "        n_epochs: (int) number of epochs performed during training\n",
        "\n",
        "    Returns:\n",
        "        best_model: (nn.Module) the trained neural network\n",
        "    \"\"\"\n",
        "    best_model = deepcopy(model)\n",
        "    train_best_loss = np.inf\n",
        "\n",
        "    batch_size = train_loader.batch_size\n",
        "    n = 10\n",
        "\n",
        "    n_batches = n//batch_size\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for i, data in enumerate(train_loader):\n",
        "            images, mask = data['data'], data['mask']\n",
        "            scans = torch.flatten(mask)\n",
        "            outputs = torch.reshape(model(images), (scans.shape[0],4))\n",
        "            loss = criterion(outputs, scans)\n",
        "            loss.backward()\n",
        "            total_loss += loss.item()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        mean_loss = total_loss / len(train_loader.dataset)\n",
        "        print('Epoch %i: loss = %f & accuracy = %f' % (epoch, mean_loss,torch.sum(torch.argmax(outputs,1) == scans)/scans.shape[0]))\n",
        "\n",
        "        if mean_loss < train_best_loss:\n",
        "            best_model = deepcopy(model)\n",
        "            train_best_loss = mean_loss\n",
        "    \n",
        "    return best_model\n",
        "\n",
        "def test(model, data_loader, criterion):\n",
        "    \"\"\"\n",
        "    Method used to test a CNN\n",
        "    \n",
        "    Args:\n",
        "        model: (nn.Module) the neural network\n",
        "        data_loader: (DataLoader) a DataLoader wrapping a MRIDataset\n",
        "        criterion: (nn.Module) a method to compute the loss of a mini-batch of images\n",
        "    \n",
        "    Returns:\n",
        "        results_df: (DataFrame) the label predicted for on the slice level.\n",
        "        results_metrics: (dict) a set of metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    columns = [\"participant_id\", \"slice_id\", \"proba0\", \"proba1\",\n",
        "               \"true_label\", \"predicted_label\"]\n",
        "    results_df = pd.DataFrame(columns=columns)\n",
        "    total_loss = 0\n",
        "\n",
        "    batch_size = data_loader.batch_size\n",
        "    n = 10000\n",
        "\n",
        "    n_batches = n//batch_size\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(data_loader, 0):\n",
        "             images, mask = data['data'], data['mask']\n",
        "             scans = torch.flatten(mask)\n",
        "             outputs = torch.reshape(model(images), (scans.shape[0],4))\n",
        "             loss = criterion(outputs, scans)\n",
        "             total_loss += loss.item()\n",
        "             print(\"Batch {}/{} loss : {}\".format(i+1, n_batches, str(total_loss/((i+1)*batch_size))))\n",
        "       #     probs = nn.Softmax(dim=1)(outputs)\n",
        "        #    _, predicted = torch.max(outputs.data, 1)\n",
        "             print(\"Accuracy : {}\".format(str(int(torch.sum(torch.argmax(outputs,1) == scans))/scans.shape[0])))\n",
        "\n",
        "    print(\"Final loss : {}\".format(str(total_loss/ len(data_loader.dataset))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krUZkF84UOz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################## Modèle ############################\n",
        "\n",
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.lin1 = nn.Linear(55, 200)\n",
        "        self.lin2 = nn.Linear(200, 200)\n",
        "        self.lin3 = nn.Linear(200, 200)\n",
        "        self.lin4 = nn.Linear(200, 4)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin1(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.lin2(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.lin3(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.lin4(x)\n",
        "        return self.relu(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEVKRJieJtJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################## Prétraitement ############################\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "database = OurDataset(path, Test, MSI, Truth, 10, 0.2)\n",
        "dataloader_train = DataLoader(database, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "model = Model()\n",
        "criterion = nn.MultiMarginLoss(p=2)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "n_epochs = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsztqK9WQfRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################### Entrainement du modèle ##########################\n",
        "\n",
        "train(model, dataloader_train, criterion, optimizer, n_epochs)\n",
        "torch.save(model.state_dict(), \"model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-4hoPV4KJ20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################### Statistiques ################################\n",
        "\n",
        "#Predict for test data \n",
        "\n",
        "database.set_mode(\"test\")\n",
        "dataloader_val = DataLoader(database, batch_size=batch_size, drop_last=True)\n",
        "\n",
        "results_metrics = test(model, dataloader_val, criterion)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}